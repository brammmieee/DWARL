envs: ???                   # number of parallel environments (and Webots instances)
policy_type: MlpPolicy      # policy type for the neural network
activation_fn: ReLU         # activation function for the neural network
net_arch:                         # network architecture for the neural network
  pi: [256, 256, 256, 256, 256]     # policy network architecture
  vf: [256, 256, 256, 256, 256]     # value function network architecture

PPO: # [1] https://arxiv.org/html/2407.14262v1
  learning_rate: 1e-4       # Reduced for more stable learning[1]
  n_steps: 4096             # Increased buffer size for more stable training[1]
  batch_size: 128           # Increased from default for better stability[1]
  n_epochs: 7               # Reduced for more stable updates[1]
  gamma: 0.99               # Standard discount factor[1]
  gae_lambda: 0.95          # Standard GAE parameter[1]
  clip_range: 0.2           # Standard clipping parameter[1]
  clip_range_vf: null       # No value function clipping
  normalize_advantage: true # Helps with training stability[1]
  ent_coef: 0.01            # Increased entropy for better exploration[1]
  vf_coef: 0.5              # Standard value function coefficient[1]
  max_grad_norm: 0.5        # Prevents exploding gradients[1]
  use_sde: true             # Enable stochastic differential exploration[1]
  sde_sample_freq: 4        # Frequency of SDE sampling[1]
