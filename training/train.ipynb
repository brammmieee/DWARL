{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib qt\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('training')\n",
    "\n",
    "import yaml\n",
    "import tensorrt\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "import utils.custom_tools as ct\n",
    "from environments.custom_env import CustomEnv\n",
    "\n",
    "from stable_baselines3.td3 import TD3\n",
    "from stable_baselines3.td3.policies import MultiInputPolicy\n",
    "from stable_baselines3.ppo import PPO\n",
    "from stable_baselines3.ppo.policies import MultiInputPolicy\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize, VecMonitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "package_dir = os.path.abspath(os.pardir)\n",
    "params_dir = os.path.join(package_dir, 'parameters')\n",
    "monitor_dir = os.path.join(os.getcwd(), 'monitor')\n",
    "\n",
    "parameters_file = os.path.join(params_dir, 'parameters.yml')\n",
    "with open(parameters_file, \"r\") as file:\n",
    "    params = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an custom gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(render_mode=None, wb_open=True, wb_mode='training')\n",
    "check_env(env)\n",
    "env = TimeLimit(env, max_episode_steps=(params['max_ep_time']/params['sample_time']))\n",
    "env = Monitor(\n",
    "    env=env,\n",
    "    filename=None,\n",
    "    info_keywords=(),  # can be used for logging the parameters for each test run for instance\n",
    ")\n",
    "# vec_env = DummyVecEnv([lambda: env])\n",
    "# vec_env = VecNormalize(\n",
    "#     venv=vec_env,\n",
    "#     training=True,\n",
    "#     norm_obs=True,\n",
    "#     norm_reward=True,\n",
    "#     clip_obs=10.0,\n",
    "#     clip_reward=10.0,\n",
    "#     gamma=0.99,\n",
    "#     epsilon=1e-8,\n",
    "#     norm_obs_keys=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model #NOTE: alternatively run the \"Loading non-archived model section!\"\n",
    "model_name = ct.get_file_name_with_date(test_nr_today=0, comment='test')\n",
    "\n",
    "# policy_kwargs = dict(net_arch=dict(pi=[120, 120, 120], vf=[120, 120, 120]))\n",
    "# Create the agent\n",
    "model = PPO(\n",
    "    policy=MultiInputPolicy,\n",
    "    env=env,\n",
    "    tensorboard_log = \"./logs/\" + model_name,\n",
    "    # policy_kwargs = policy_kwargs,\n",
    "    # learning_rate= 1e-4,\n",
    "    # n_steps = 4000, # increase the\n",
    "    # batch_size = 500, # increase batch size\n",
    "    # n_epochs = 10,\n",
    "    # gamma = 0.999, # more emphasis on future rewards\n",
    "    # ent_coef = 0.01, # increase exploration\n",
    ")\n",
    "\n",
    "# %% Callbacks\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq = 10000,\n",
    "    save_path = \"./models/\" + model_name,\n",
    "    name_prefix = model_name,\n",
    "    save_replay_buffer = False,\n",
    "    save_vecnormalize = False,\n",
    "    verbose = 0,\n",
    ")\n",
    "# stop_training_callback = StopTrainingOnNoModelImprovement(\n",
    "#     max_no_improvement_evals = 10,\n",
    "#     min_evals = 25,\n",
    "#     verbose = 1,\n",
    "# )\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env = env,\n",
    "    callback_on_new_best = None,\n",
    "    callback_after_eval = None,\n",
    "    n_eval_episodes = 25,\n",
    "    eval_freq = 15000,\n",
    "    log_path = None,\n",
    "    best_model_save_path = \"./models\",\n",
    "    deterministic = False,\n",
    "    render = False,\n",
    "    verbose = 0,\n",
    "    warn = True,\n",
    ")\n",
    "callback_list = CallbackList([checkpoint_callback, eval_callback]) #NOTE: can also pass list directly to learn\n",
    "\n",
    "# %% Train model\n",
    "model.learn(\n",
    "    total_timesteps=1e8,\n",
    "    callback=callback_list,\n",
    "    log_interval=10,\n",
    "    tb_log_name=model_name,\n",
    "    reset_num_timesteps=False,\n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading an archived model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Name\n",
    "model_name = 'insert model name'\n",
    "n_steps_load = 00000000\n",
    "\n",
    "# %% Load model\n",
    "model = PPO.load('./models' + '/' + model_name +  '/' + model_name + '_' + str(n_steps_load) + '_steps')\n",
    "\n",
    "# %% Set new env\n",
    "model.set_env(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(render_mode='full', wb_open=True, wb_mode='testing', reward_monitoring=False)\n",
    "\n",
    "# %% Load model\n",
    "model = PPO.load('./models' + '/insert model name')\n",
    "\n",
    "# %% Setting render mode and eval vars\n",
    "env.set_render_mode('full')\n",
    "nr_eval_eps = 10\n",
    "\n",
    "# %% Evaluation runs\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    env=env,\n",
    "    n_eval_episodes=nr_eval_eps,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    callback=None,\n",
    "    reward_threshold=None,\n",
    "    return_episode_rewards=False,\n",
    "    warn=True,\n",
    ")\n",
    "print(f'mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring and plotting reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# %%\n",
    "from environments.custom_env import CustomEnv\n",
    "from utils import custom_tools as ct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %%\n",
    "env = CustomEnv(render_mode='trajectory', wb_open=True, wb_mode='testing', reward_monitoring=True)\n",
    "nr_eps = 1\n",
    "\n",
    "# %%\n",
    "ep_reward_list = []\n",
    "ep_reward = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # action, _ = model.predict(obs, deterministic=True)\n",
    "    # action = env.action_space.sample()\n",
    "    action = np.array([1.0, -1.0])\n",
    "\n",
    "    # assert env.action_space.contains(action)\n",
    "    # print(f'action = {action}')\n",
    "\n",
    "    obs, reward, done, info = env.step(action, teleop=True)\n",
    "    # print(f'reward = {reward}')\n",
    "    ep_reward += reward\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        ep_reward_list.append(ep_reward)\n",
    "        ep_reward = 0\n",
    "        if len(ep_reward_list) >= nr_eps:\n",
    "            break\n",
    "\n",
    "# %%\n",
    "# NOTE: use the file_name of with the max ep idx\n",
    "reward_matrix = ct.read_pickle_file(file_name='rewards_ep_1', file_dir=os.path.join('training','rewards'))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('timestep')\n",
    "ax.set_ylabel('reward')\n",
    "ax.grid()\n",
    "\n",
    "for ep_rewards in reward_matrix[:]:\n",
    "    ax.plot(list(range(0, len(ep_rewards),1)), ep_rewards)\n",
    "\n",
    "ax.legend([f'ep_{ep_nr+1}' for ep_nr in range(len(reward_matrix))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# %%\n",
    "from environments.custom_env import CustomEnv\n",
    "import numpy as np\n",
    "\n",
    "# %%\n",
    "env = CustomEnv(render_mode='full', wb_open=True, wb_mode='testing', reward_monitoring=False)\n",
    "\n",
    "# %%\n",
    "obs = env.reset() #options={\"map_nr\":40, \"nominal_dist\":1})\n",
    "\n",
    "# %%\n",
    "action = np.array([1.0, 1.0])\n",
    "obs, reward, done, _, _ = env.step(action, teleop=False)\n",
    "\n",
    "# %%\n",
    "import time\n",
    "\n",
    "# env.set_render_mode('velocity')\n",
    "# obs = env.reset()\n",
    "n_steps = 100000000\n",
    "for i in range(n_steps):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action, teleop=True)\n",
    "    # time.sleep(0.4)\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the map resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.map2proto import Map2ProtoConverter\n",
    "\n",
    "# %%\n",
    "m2p = Map2ProtoConverter()\n",
    "\n",
    "# %%\n",
    "m2p.convert(map_res = params['map_res'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training map lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create the training and test map lists\n",
    "train_map_nr_list, test_map_nr_list = ct.sample_training_test_map_nrs(range_start=0, range_end=299, training_ratio=0.7)\n",
    "\n",
    "# %% Save map lists to pickle files #NOTE: proceed with caution, files will be overwritten\n",
    "ct.write_pickle_file('train_map_nr_list', 'parameters', train_map_nr_list)\n",
    "ct.write_pickle_file('test_map_nr_list', 'parameters', test_map_nr_list)\n",
    "\n",
    "# %% Check if everything went well\n",
    "train_map_nr_list = ct.read_pickle_file('train_map_nr_list', 'parameters')\n",
    "test_map_nr_list = ct.read_pickle_file('test_map_nr_list', 'parameters')\n",
    "print(f'train_map_nr_list = {train_map_nr_list}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
